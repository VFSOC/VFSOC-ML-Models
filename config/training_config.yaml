# High-Accuracy Training Configuration for GPS Jamming Detection
# Optimized for maximum accuracy with overfitting prevention

# Dataset Configuration
dataset:
  target_samples: 65000  # Increased for better generalization
  jamming_ratio: 0.08    # 8% realistic distribution
  enhanced_features: true
  quality_enhancements: true
  
  # Scenario diversity for robust training
  scenario_distribution:
    urban_scenarios: 0.35      # City driving with frequent stops
    highway_scenarios: 0.25    # High-speed continuous driving  
    rural_scenarios: 0.20      # Variable terrain and coverage
    industrial_scenarios: 0.20 # High interference environments
  
  # Temporal pattern distribution
  temporal_patterns:
    morning_rush: 0.25     # 7-9 AM high traffic
    afternoon_rush: 0.25   # 5-7 PM high traffic
    business_hours: 0.30   # 9 AM - 5 PM normal operations
    off_hours: 0.20        # Evenings, nights, weekends
  
  # Attack pattern variety for comprehensive detection
  attack_patterns:
    simple_jamming: 0.40   # Basic continuous interference
    sweep_jamming: 0.25    # Frequency sweeping attacks
    pulse_jamming: 0.20    # Intermittent bursts
    chirp_jamming: 0.15    # Frequency modulated attacks

# Anti-Overfitting Configuration
anti_overfitting:
  validation_strategy: "stratified_kfold"
  cv_folds: 10                    # Extensive cross-validation
  cv_repeats: 3                   # Multiple runs for stability
  test_size: 0.15                 # Hold-out test set
  validation_size: 0.15           # Validation set size
  
  # Regularization settings
  feature_selection_ratio: 0.8    # Use top 80% of features
  early_stopping_rounds: 50       # Patience for early stopping
  regularization_strength: "adaptive"
  
  # Ensemble diversity requirements
  ensemble_diversity_threshold: 0.3
  ensemble_min_models: 3
  ensemble_max_models: 7

# Model Configuration
models:
  # Random Forest - Primary tree-based model
  random_forest:
    enabled: true
    priority: 1
    base_params:
      n_estimators: 300          # Increased for stability
      max_depth: 20              # Allow deeper trees but control overfitting
      min_samples_split: 15      # Higher threshold to prevent overfitting
      min_samples_leaf: 7        # Minimum samples per leaf
      max_features: "sqrt"       # Feature subsampling
      bootstrap: true
      oob_score: true           # Out-of-bag validation
      class_weight: "balanced"  # Handle class imbalance
      random_state: 42
      n_jobs: -1
    
    hyperparameter_search:
      n_estimators: [250, 300, 400]
      max_depth: [15, 20, 25]
      min_samples_split: [10, 15, 20]
      min_samples_leaf: [5, 7, 10]
      max_features: ["sqrt", "log2", 0.8]
  
  # Gradient Boosting - Advanced boosting with regularization
  gradient_boosting:
    enabled: true
    priority: 2
    base_params:
      n_estimators: 300
      learning_rate: 0.05        # Lower learning rate for better generalization
      max_depth: 6
      min_samples_split: 15
      min_samples_leaf: 7
      subsample: 0.8            # Stochastic gradient boosting
      max_features: "sqrt"
      validation_fraction: 0.1  # For early stopping
      n_iter_no_change: 15      # Early stopping patience
      random_state: 42
    
    hyperparameter_search:
      n_estimators: [250, 300, 400]
      learning_rate: [0.03, 0.05, 0.08]
      max_depth: [4, 6, 8]
      subsample: [0.7, 0.8, 0.9]
      min_samples_split: [10, 15, 20]
  
  # XGBoost - High-performance gradient boosting
  xgboost:
    enabled: true
    priority: 3
    base_params:
      n_estimators: 300
      learning_rate: 0.05
      max_depth: 6
      min_child_weight: 5        # Higher for regularization
      subsample: 0.8
      colsample_bytree: 0.8      # Feature subsampling
      reg_alpha: 0.1             # L1 regularization
      reg_lambda: 1.5            # L2 regularization
      scale_pos_weight: 12       # Handle severe class imbalance
      random_state: 42
      n_jobs: -1
      eval_metric: "logloss"
      early_stopping_rounds: 20
    
    hyperparameter_search:
      n_estimators: [250, 300, 400]
      learning_rate: [0.03, 0.05, 0.08]
      max_depth: [4, 6, 8]
      min_child_weight: [3, 5, 7]
      reg_alpha: [0.0, 0.1, 0.3]
      reg_lambda: [1.0, 1.5, 2.0]
  
  # LightGBM - Fast gradient boosting
  lightgbm:
    enabled: true
    priority: 4
    base_params:
      n_estimators: 300
      learning_rate: 0.05
      max_depth: 6
      min_child_samples: 20      # Regularization
      subsample: 0.8
      colsample_bytree: 0.8
      reg_alpha: 0.1
      reg_lambda: 1.0
      scale_pos_weight: 12
      random_state: 42
      n_jobs: -1
      early_stopping_rounds: 20
    
    hyperparameter_search:
      n_estimators: [250, 300, 400]
      learning_rate: [0.03, 0.05, 0.08]
      max_depth: [4, 6, 8]
      num_leaves: [31, 63, 127]
      min_child_samples: [15, 20, 25]
  
  # Isolation Forest - Anomaly detection approach
  isolation_forest:
    enabled: true
    priority: 5
    base_params:
      n_estimators: 300
      contamination: 0.08        # Expected anomaly rate
      max_samples: "auto"
      max_features: 1.0
      bootstrap: false
      random_state: 42
      n_jobs: -1
    
    hyperparameter_search:
      n_estimators: [250, 300, 400]
      contamination: [0.06, 0.08, 0.10, 0.12]
      max_features: [0.7, 0.8, 0.9, 1.0]
      max_samples: ["auto", 0.8, 0.9]

# Hyperparameter Optimization
hyperparameter_optimization:
  method: "bayesian"              # Bayesian optimization for efficiency
  n_iter: 40                      # Number of optimization iterations
  cv_folds: 5                     # Cross-validation for optimization
  scoring: "f1"                   # Primary metric for optimization
  n_jobs: -1                      # Parallel processing
  
  # Fallback to random search if Bayesian not available
  fallback_method: "random_search"
  fallback_n_iter: 30

# Performance Targets
performance_targets:
  minimum_f1_score: 0.92          # High F1 target
  minimum_precision: 0.94         # High precision to minimize false positives
  minimum_recall: 0.90            # High recall to catch jamming attacks
  minimum_roc_auc: 0.95           # Strong discrimination capability
  maximum_false_positive_rate: 0.03  # Very low false positive tolerance
  maximum_inference_time_ms: 100  # Real-time performance requirement

# Training Strategy
training_strategy:
  max_training_iterations: 5      # Multiple training attempts if needed
  convergence_patience: 3         # Iterations to wait for improvement
  performance_improvement_threshold: 0.01  # Minimum improvement to continue
  
  # Progressive training approach
  progressive_training:
    enabled: true
    start_samples: 30000          # Start with smaller dataset
    increment_samples: 10000      # Increase gradually
    max_samples: 80000            # Maximum dataset size
  
  # Model selection strategy
  model_selection:
    primary_metric: "f1_score"
    secondary_metrics: ["precision", "recall", "roc_auc"]
    ensemble_threshold: 0.88      # Minimum performance for ensemble inclusion
    diversity_weight: 0.3         # Weight for model diversity in ensemble

# Feature Engineering
feature_engineering:
  # Statistical features
  statistical_features:
    enabled: true
    rolling_windows: [5, 10, 20]  # Multiple time windows
    statistical_measures: ["mean", "std", "min", "max", "median", "skew"]
  
  # Temporal features
  temporal_features:
    enabled: true
    time_of_day_bins: 24
    day_of_week_encoding: true
    seasonal_features: true
    rush_hour_detection: true
  
  # Geographic features
  geographic_features:
    enabled: true
    clustering_algorithm: "kmeans"
    n_clusters: 15
    distance_features: true
    density_features: true
  
  # Signal processing features
  signal_features:
    enabled: true
    signal_strength_ratios: true
    satellite_reliability_scores: true
    gps_quality_indices: true
    interference_indicators: true

# Data Quality
data_quality:
  # Outlier detection and handling
  outlier_detection:
    enabled: true
    method: "isolation_forest"
    contamination: 0.05
    action: "flag"  # flag, remove, or transform
  
  # Missing value handling
  missing_values:
    strategy: "median"  # median, mean, or knn
    indicator: true     # Add missing value indicator features
  
  # Data validation
  validation:
    feature_range_checks: true
    correlation_analysis: true
    distribution_analysis: true
    class_balance_checks: true

# Logging and Monitoring
logging:
  level: "INFO"
  file_logging: true
  console_logging: true
  detailed_metrics: true
  
  # Progress tracking
  progress_reporting:
    enabled: true
    interval_seconds: 300        # Report progress every 5 minutes
    metrics_to_track: ["f1_score", "precision", "recall", "training_time"]

# Output Configuration
output:
  base_directory: "results/high_accuracy_training"
  
  # Save configuration
  save_models: true
  save_predictions: true
  save_feature_importance: true
  save_training_history: true
  save_cross_validation_results: true
  
  # Model export formats
  export_formats: ["pkl", "joblib"]  # Can add "onnx" for production
  
  # Visualization
  generate_plots: true
  plot_types: ["confusion_matrix", "roc_curve", "feature_importance", "learning_curve"]

# Resource Management
resources:
  # Memory management
  memory_optimization: true
  batch_processing: true
  max_memory_usage_gb: 8
  
  # CPU utilization
  n_jobs: -1                     # Use all available cores
  parallel_backend: "threading"  # or "multiprocessing"
  
  # GPU acceleration (if available)
  use_gpu: false                 # Set to true if GPU libraries available
  gpu_memory_fraction: 0.8

# Reproducibility
reproducibility:
  random_state: 42
  numpy_seed: 42
  deterministic_algorithms: true
  save_random_states: true 